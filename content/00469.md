---
title: Pivotal Container Service (PKS)にService Catalogをインストールするworkaround
tags: ["Kubernetes", "PKS", "Service Catalog", "Open Service Broker API", "BOSH", "CFCR"]
categories: ["Dev", "CaaS", "Kubernetes", "PKS"]
---

以前k8s meetupで話した[Service Catalog](https://svc-cat.io)ですが、執筆現時点ではPKS上で使うにはちょっとしたhackを行う必要があったので、
その方法をメモします。CFCRでも基本的には同じです。

以前の資料はこちら。

<iframe src="//www.slideshare.net/slideshow/embed_code/key/Dve9oayyVTpw01" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/makingx/open-service-broker-apikubernetes-service-catalog-k8sjp-90024385" title="Open Service Broker APIとKubernetes Service Catalog #k8sjp" target="_blank">Open Service Broker APIとKubernetes Service Catalog #k8sjp</a> </strong> from <strong><a href="https://www.slideshare.net/makingx" target="_blank">Toshiaki Maki</a></strong> </div>


### Helmのインストール

Helmは好みではないのですが、現状Service CatalogをインストールするにはHelmを使うのが一番簡単なのでこの記事ではHelmを使用します。
Helmインストール済みの場合はskipしてください。


まずはTillerをデプロイするための`ServiceAccout`と`ClusterRoleBinding`の設定をします。

```yaml
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF
```


`helm` CLIをインストールします。

```
brew install kubernetes-helm
```

Tillerをインストールします。

```
helm init --service-account tiller
```


### Service Catalogのインストール

Service Catalog用のレポジトリを追加します。

```
helm repo add svc-cat https://svc-catalog-charts.storage.googleapis.com
```

本記事ではバージョン`0.1.38`を使用します。

```
$ helm search service-catalog
NAME           	CHART VERSION	APP VERSION	DESCRIPTION                                                 
svc-cat/catalog	0.1.38       	           	service-catalog API server and controller-manager helm chart
```

`catalog` namespaceにService Catalogをインストールします。

```
helm install svc-cat/catalog \
    --name catalog --namespace catalog
```

### Workaroundの一時適用

実はPKS上ではこのままでは動きません。`catalog-controller-manager`がCrashするはずです。次のようなログが出力されます。

```
W0105 15:09:48.062514       1 feature_gate.go:198] Setting GA feature gate OriginatingIdentity=true. It will be removed in a future release.
I0105 15:09:48.062663       1 feature_gate.go:206] feature gates: &{map[OriginatingIdentity:true]}
I0105 15:09:48.062747       1 feature_gate.go:206] feature gates: &{map[OriginatingIdentity:true ServicePlanDefaults:false]}
I0105 15:09:48.062813       1 hyperkube.go:192] Service Catalog version v0.1.38 (built 2018-11-09T01:46:26Z)
I0105 15:09:48.062905       1 controller_manager.go:105] Building k8s kubeconfig
I0105 15:09:48.065019       1 controller_manager.go:133] Building service-catalog kubeconfig for url: 
I0105 15:09:48.065067       1 controller_manager.go:140] Using inClusterConfig to talk to service catalog API server -- make sure your API server is registered with the aggregator
I0105 15:09:48.065286       1 controller_manager.go:161] Starting http server and mux
I0105 15:09:48.065399       1 controller_manager.go:193] Creating event broadcaster
I0105 15:09:48.065553       1 healthz.go:115] Installing healthz checkers:"ping", "checkAPIAvailableResources"
I0105 15:09:48.065634       1 metrics.go:92] Registered /metrics with prometheus
I0105 15:09:48.066040       1 controller_manager.go:295] Created client for API discovery
I0105 15:09:48.066181       1 round_trippers.go:386] curl -k -v -XGET  -H "User-Agent: service-catalog/v0.1.38 (linux/amd64) kubernetes/19c1ae7/service-catalog-controller-discovery" -H "Accept: application/json, */*" 'https://10.100.200.1:443/apis/servicecatalog.k8s.io/v1beta1?timeout=32s'
I0105 15:09:48.166743       1 round_trippers.go:405] GET https://10.100.200.1:443/apis/servicecatalog.k8s.io/v1beta1?timeout=32s 401 Unauthorized in 100 milliseconds
I0105 15:09:48.166765       1 round_trippers.go:411] Response Headers:
I0105 15:09:48.166772       1 round_trippers.go:414]     Audit-Id: 0312d47a-c9b8-47cc-ba38-76677eaccd5d
I0105 15:09:48.166778       1 round_trippers.go:414]     Content-Type: application/json
I0105 15:09:48.166789       1 round_trippers.go:414]     Date: Sat, 05 Jan 2019 15:09:48 GMT
I0105 15:09:48.166795       1 round_trippers.go:414]     Content-Length: 129
I0105 15:09:48.166839       1 request.go:942] Response Body: {"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"Unauthorized","reason":"Unauthorized","code":401}
F0105 15:09:48.167361       1 controller_manager.go:233] error running controllers: failed to get api versions from server: failed to get supported resources from server: Unauthorized
```

Service Catalogは現時点ではAPIの提供にCustom Resource Definition(CRD)ではなく[Aggregation Layer](https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/)を使用しており、Mutual TLSで認証されています。

Service Catalogのドキュメントの[Before you begin](https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/#before-you-begin)に次のように書かれています。

> Note: There are a few setup requirements for getting the aggregation layer working in your environment to support mutual TLS auth between the proxy and extension apiservers. Kubernetes and the kube-apiserver have multiple CAs, so make sure that the proxy is signed by the aggregation layer CA and not by something else, like the master CA.

PKSで作成されたk8sクラスタのAPI Serverに設定されている起動オプションは次のコマンドで確認できます。

```
bosh -d service-instance_<instance uuid> ssh master -c "cat /var/vcap/jobs/kube-apiserver/config/bpm.yml"
```

`<instance_uuid>`は`pks cluster <cluster_name>`または`bosh deployments`で確認できます。

次のような出力結果になります。

```yaml
processes:
- name: kube-apiserver
  executable: /var/vcap/packages/kubernetes/bin/kube-apiserver
  args:
  - --anonymous-auth=true
  - --allow-privileged=false
  - --apiserver-count=1
  - --bind-address=0.0.0.0
  - --cloud-provider=vsphere
  - --cloud-config=/var/vcap/jobs/kube-apiserver/config/cloud-provider.ini
  - --kubelet-client-key=/var/vcap/jobs/kube-apiserver/config/
  - --kubelet-client-certificate=/var/vcap/jobs/kube-apiserver/config/
  - --enable-admission-plugins=LimitRanger,DefaultTolerationSeconds,ValidatingiceAccount,DefaultStorageClass,MutatingAdmissionWebhook,DenyEscalatingExec,Sec
  - --disable-admission-plugins=PersistentVolumeLabel
  - --enable-bootstrap-token-auth
  - --enable-swagger-ui
  - --etcd-servers=https://master-0.etcd.cfcr.internal:2379
  - --etcd-cafile=/var/vcap/jobs/kube-apiserver/config/etcd-ca.crt
  - 
  - --etcd-certfile=/var/vcap/jobs/kube-apiserver/config/etcd-client.crt
  - --etcd-keyfile=/var/vcap/jobs/kube-apiserver/config/etcd-client.key
  - --kubelet-https
  - --secure-port=8443
  - --service-account-key-file=/var/vcap/jobs/kube-apiserver/config/
  - --service-account-lookup
  - --service-cluster-ip-range=10.100.200.0/24
  - --service-node-port-range=30000-32767
  - --tls-cert-file=/var/vcap/jobs/kube-apiserver/config/kubernetes.pem
  - --tls-private-key-file=/var/vcap/jobs/kube-apiserver/config/
  - --token-auth-file=/var/vcap/jobs/kube-apiserver/config/tokens.csv
  - --storage-media-type=application/json
  - --enable-aggregator-routing
  - --v=2
  - --authorization-mode=RBAC
  - --runtime-config=api/v1
  - --client-ca-file=/var/vcap/jobs/kube-apiserver/config/kubernetes.pem
  - --proxy-client-cert-file=/var/vcap/jobs/kube-apiserver/config/
  - --proxy-client-key-file=/var/vcap/jobs/kube-apiserver/config/
  - --requestheader-allowed-names=aggregator
  - --requestheader-client-ca-file=/var/vcap/jobs/kube-apiserver/config/
  - --requestheader-extra-headers-prefix=X-Remote-Extra-
  - --requestheader-group-headers=X-Remote-Group
  - --requestheader-username-headers=X-Remote-User
  - --audit-log-path=/var/vcap/sys/log/kube-apiserver/audit.log
  - --audit-log-maxage=0
  - --audit-log-maxsize=49
  - --audit-log-maxbackup=7
  - --audit-policy-file=/var/vcap/jobs/kube-apiserver/config/audit_policy.yml
  - --oidc-issuer-url=https://api.pks.ik.am:8443/oauth/token
  - --oidc-client-id=pks_cluster_client
  - --oidc-username-claim=user_name
  - "--oidc-username-prefix=-"
  - --oidc-groups-claim=roles
  - "--oidc-groups-prefix="
  - --oidc-ca-file=/var/vcap/jobs/kube-apiserver/config/oidc-ca.pem
  env:  
```

`--requestheader-allowed-names=aggregator`がありますが、どうも使われているCAのCNが`aggregator`でなく、API Serverのhost名、つまり`pks create-cluster <cluster_name> -e <host_name>`で指定した`<host_name>`になっています。
上の例では`demo.pks.ik.am`です。

この場合、`--requestheader-allowed-names=aggregator,demo.pks.ik.am`にしてAPI Serverを再起動すればうまく動きます。

次のコマンドで`requestheader-allowed-names`の値を一時的に変更します。

```
bosh -d service-instance_<instance_uuid> ssh master -c "sudo sed -i 's/--requestheader-allowed-names=aggregator$/--requestheader-allowed-names=aggregator,demo.pks.ik.am/g' /var/vcap/jobs/kube-apiserver/config/bpm.yml"
```

次のコマンドでAPI Serverを再起動します。

```
bosh -d service-instance_<instance_uuid> ssh master -c "sudo /var/vcap/bosh/bin/monit restart kube-apiserver"
```

multi-master構成の場合は上記を全てのmasterに対して行います。`master/0`, `master/1`, `master/2`と言うように対象のVMを指定できます。

一旦Service Catalogのpodを削除して、再作成を待ちます。

```
kubectl -n catalog delete pod -l release=catalog
```

しばらくすると`catalog-apiserver`および`catalog-controller-manager`

```
$ kubectl get pod -n catalog 
NAME                                                  READY   STATUS    RESTARTS   AGE
catalog-catalog-apiserver-58dd7744c5-swcmg            2/2     Running   0          17h
catalog-catalog-controller-manager-755b8b54d4-97g7s   1/1     Running   2          17h
```

> 将来的にService CatalogがCRDを使う方針に変われば、このWorkaroundは不要になるかもしれません。


CFCR(v0.22.0+)の場合は次のようなops-fileを適用すれば良いです。

```yaml
- type: replace
  path: /instance_groups/name=master/jobs/name=kube-apiserver/properties/k8s-args/requestheader-allowed-names
  value: aggregator,demo.pks.ik.am
```

### Workaroundの恒久適用
 
上記のようにVMにsshして修正するworkaroundは、一時的にしか効きません。
BOSHはimmutableなinfrastructureを提供するため、updateやrecreateのタイミングで元の設定に戻ります。

workaroundを恒久的に行うにはBOSHのライフサイクルのタイミングに合わせて`sed`を行うさらなるhackが必要があります。

これはあとで書来ます。

### サンプルService Brokerのデプロイ

サンプルとしてKafka as a ServiceのCloud KarafkaのインスタンスをプロビジョンしてくれるService Brokerをデプロイしてみます。

https://github.com/making/cloud-karafka-service-broker


`svcat` CLIをインストールします。

```
brew install kubernetes-service-catalog-client
```


