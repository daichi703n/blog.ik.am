---
title: Tanzu Kubernetes Grid 1.1をvSphere 6.7にインストールするメモ
tags: ["Kubernetes", "vSphere", "TKG", "Tanzu", "Cluster API", "MetalLB"]
categories: ["Dev", "CaaS", "Kubernetes", "TKG", "vSphere"]
---

本記事は、[Tanzu Kubernetes Grid (TKG)](https://docs.vmware.com/jp/VMware-Tanzu-Kubernetes-Grid/index.html) 1.1.3をvSphereにインストールするメモです。

> [Tanzu Kubernetes Grid 1.1をAWSにインストールするメモ](https://blog.ik.am/entries/543)のvSphere版です。

TKGは[Cluster API](https://cluster-api.sigs.k8s.io)(`clusterctl`)のラッパーのようなプロダクトです。

Cluster APIには

* Management Cluster
* Workload Cluster

という2種類のKubernetes Clusterがあります。

Workload ClusterはManagement Clusterによってライフサイクルを管理されます。<br>
つまり、Kubernetesで管理されるKubernetes Clusterです。

では、Kubernetesを管理するKubernetesは誰が管理するのでしょうか?

...Kubernetesです😅 <br>
TKGではManagement Clusterは[Kind](https://kind.sigs.k8s.io)によってbootstrapされます。

つまり Kind -> Management Cluster -> Workload Clusterという親子関係でKubernetes Clusterが管理されるため、
Cluster APIは次のような亀のロゴになっています。

<img src="https://user-images.githubusercontent.com/106908/90330477-e7588900-dfe7-11ea-86a6-91533b0b2fa1.png" width="200" />

Cluster APIについてより知りたい場合は、↓の資料や

* https://www.cncf.io/wp-content/uploads/2020/06/Cluster-API-CNCF-Webinar-20200611.pdf

次のKubeAcademyの動画を見ると良いです。

* https://kube.academy/lessons/bootstrapping-cluster-api-part-1-concepts-components-and-terminology
* https://kube.academy/lessons/bootstrapping-cluster-api-part-2-creating-a-cluster-on-aws-with-cluster-api

> vSphere 7の場合は[TKG Service](https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-kubernetes/GUID-7E00E7C2-D1A1-4F7D-9110-620F30C02547.html)を有効にすることで、Management Clusterに相当する、Supervisor Clusterが手に入るので、この手順でTKGをインストールする必要はありません。

**Table of contents**
<!-- toc -->

### CLIのインストール

[https://my.vmware.com/en/web/vmware/downloads/details?downloadGroup=TKG-113&productId=988&rPId=48121](https://my.vmware.com/en/web/vmware/downloads/details?downloadGroup=TKG-113&productId=988&rPId=48121)

から

- VMware Tanzu Kubernetes Grid CLI for Mac

をダウンロードして次のようにインストールしてください。

```
gunzip tkg-darwin-amd64-v1.1.3-vmware.1.gz

mv tkg-darwin-amd64-v1.1.3-vmware.1 /usr/local/bin/tkg

chmod +x /usr/local/bin/tkg
```

その他、Dockerが必要です。

次のバージョンで動作確認しています。

```
$ tkg version
Client:
	Version: v1.1.3
	Git commit: 0e8e58f3363a1d4b4063b9641f44a3172f6ff406
```

次のコマンドを実行すると設定ファイル用のディレクトリが生成されます。

```
tkg get management-cluster
```

次のようなディレクトリが作成します。

```
$ find ~/.tkg 
/Users/toshiaki/.tkg
/Users/toshiaki/.tkg/bom
/Users/toshiaki/.tkg/bom/bom-1.1.2+vmware.1.yaml
/Users/toshiaki/.tkg/bom/bom-tkg-1.0.0.yaml
/Users/toshiaki/.tkg/bom/bom-1.17.6+vmware.1.yaml
/Users/toshiaki/.tkg/bom/bom-1.1.0+vmware.1.yaml
/Users/toshiaki/.tkg/bom/bom-1.17.9+vmware.1.yaml
/Users/toshiaki/.tkg/bom/bom-1.1.3+vmware.1.yaml
/Users/toshiaki/.tkg/providers
/Users/toshiaki/.tkg/providers/control-plane-kubeadm
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.6
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.6/control-plane-components.yaml
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.5
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.5/control-plane-components.yaml
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.3
/Users/toshiaki/.tkg/providers/control-plane-kubeadm/v0.3.3/control-plane-components.yaml
/Users/toshiaki/.tkg/providers/providers.md5sum
/Users/toshiaki/.tkg/providers/infrastructure-vsphere
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.4/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.3/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.5/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.6
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.6/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.6/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-vsphere/v0.6.6/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/cluster-api
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.6
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.6/core-components.yaml
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.5
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.5/core-components.yaml
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.3
/Users/toshiaki/.tkg/providers/cluster-api/v0.3.3/core-components.yaml
/Users/toshiaki/.tkg/providers/config.yaml
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.6
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.6/bootstrap-components.yaml
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.5
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.5/bootstrap-components.yaml
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.3
/Users/toshiaki/.tkg/providers/bootstrap-kubeadm/v0.3.3/bootstrap-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.4/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.3/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2/infrastructure-components.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-aws/v0.5.2/cluster-template-prod.yaml
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere/v1.0.0
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere/v1.0.0/cluster-template-dev.yaml
/Users/toshiaki/.tkg/providers/infrastructure-tkg-service-vsphere/v1.0.0/cluster-template-prod.yaml
/Users/toshiaki/.tkg/config.yaml
```

### vCenterにOVAファイルをアップロード

[https://my.vmware.com/en/web/vmware/downloads/details?downloadGroup=TKG-113&productId=988&rPId=48121](https://my.vmware.com/en/web/vmware/downloads/details?downloadGroup=TKG-113&productId=988&rPId=48121)

から

- Photon v3 Kubernetes v1.18.6 OVA
- Photon v3 capv haproxy v1.2.4 OVA

をダウンロードし、`~/Downloads`に保存します。

TKGをインストールする環境の情報を次の例のように設定してください。インストール先のネットワークは**DHCPが有効になっている必要があります**。

```
cat <<EOF > tkg-env.sh
export GOVC_URL=vcsa-01.haas-409.pez.vmware.com
export GOVC_USERNAME=administrator@vsphere.local
export GOVC_PASSWORD=VMware1!
export GOVC_DATACENTER=Datacenter
export GOVC_NETWORK=Extra
export GOVC_DATASTORE=LUN01
export GOVC_RESOURCE_POOL=/Datacenter/host/Cluster/Resources/tkg
export GOVC_INSECURE=1
export TEMPLATE_FOLDER=/Datacenter/vm/tkg
EOF
```

次のコマンドでOVAファイルをvCenterにアップロードします。

```
source tkg-env.sh
govc pool.create /Datacenter/host/Cluster/Resources/tkg
govc folder.create /Datacenter/vm/tkg

govc import.ova -folder $TEMPLATE_FOLDER ~/Downloads/photon-3-kube-v1.18.6_vmware.1.ova
govc import.ova -folder $TEMPLATE_FOLDER ~/Downloads/photon-3-haproxy-v1.2.4-vmware.1.ova

govc vm.markastemplate $TEMPLATE_FOLDER/photon-3-kube-v1.18.6
govc vm.markastemplate $TEMPLATE_FOLDER/photon-3-haproxy-v1.2.4
```

### SSH Keyの作成

TKGで作成されるVM用のSSH Keyを作成します。

```
export AWS_SSH_KEY_NAME=tkg-sandbox
aws ec2 create-key-pair --key-name tkg-sandbox --output json | jq .KeyMaterial -r > tkg-sandbox.pem
```

```
ssh-keygen -t rsa -b 4096 -f ~/.ssh/tkg
```

キーフレーズは空にしてください。

### Management ClusterをAWSに作成

TKGでWorkload Clusterを作成するための、Management Clusterを作成します。

次のコマンドでvCenterの情報をManagement Cluster用の設定ファイル(`~/.tkg/config.yaml`)に追記します。

```
source tkg-env.sh
cat <<EOF >> ~/.tkg/config.yaml
VSPHERE_SERVER: ${GOVC_URL}
VSPHERE_USERNAME: ${GOVC_USERNAME}
VSPHERE_PASSWORD: ${GOVC_PASSWORD}
VSPHERE_DATACENTER: ${GOVC_DATACENTER}
VSPHERE_DATASTORE: ${GOVC_DATACENTER}/datastore/${GOVC_DATASTORE}
VSPHERE_NETWORK: ${GOVC_NETWORK}
SERVICE_CIDR: 100.64.0.0/13
CLUSTER_CIDR: 100.96.0.0/11
VSPHERE_RESOURCE_POOL: ${GOVC_RESOURCE_POOL}
VSPHERE_FOLDER: ${TEMPLATE_FOLDER}
VSPHERE_CONTROL_PLANE_NUM_CPUS: "2"
VSPHERE_CONTROL_PLANE_MEM_MIB: "4096"
VSPHERE_CONTROL_PLANE_DISK_GIB: "40"
VSPHERE_WORKER_NUM_CPUS: "2"
VSPHERE_WORKER_MEM_MIB: "4096"
VSPHERE_WORKER_DISK_GIB: "40"
VSPHERE_HA_PROXY_NUM_CPUS: "2"
VSPHERE_HA_PROXY_MEM_MIB: "4096"
VSPHERE_HA_PROXY_DISK_GIB: "40"
VSPHERE_HAPROXY_TEMPLATE: ${TEMPLATE_FOLDER}/photon-3-haproxy-v1.2.4
VSPHERE_SSH_AUTHORIZED_KEY: $(cat ~/.ssh/tkg.pub)
EOF
```

次のコマンドでManagement Clusterを作成します。Control Planeの台数は`dev` planが1台、`prod`が3台になります。

```
tkg init --infrastructure vsphere --name tkg-sandbox --plan dev
```

> `tkg init`でKindクラスタ作成、`clusterctl init`、`clusterclt config`、`kubectl apply`相当の処理が行われます。

次のようなログが出力されます。

```
Logs of the command execution can also be found at: /var/folders/76/vg4pyy253pbgwzncmx2mb1gh0000gq/T/tkg-20200922T194233610375911.log

Validating the pre-requisites...

Setting up management cluster...
Validating configuration...
Using infrastructure provider vsphere:v0.6.6
Generating cluster configuration...
Setting up bootstrapper...
Bootstrapper created. Kubeconfig: /Users/toshiaki/.kube-tkg/tmp/config_q0ppw4E1
Installing providers on bootstrapper...
Fetching providers
Installing cert-manager
Waiting for cert-manager to be available...
Installing Provider="cluster-api" Version="v0.3.6" TargetNamespace="capi-system"
Installing Provider="bootstrap-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-bootstrap-system"
Installing Provider="control-plane-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-control-plane-system"
Installing Provider="infrastructure-vsphere" Version="v0.6.6" TargetNamespace="capv-system"
Start creating management cluster...
Saving management cluster kuebconfig into /Users/toshiaki/.kube/config
Installing providers on management cluster...
Fetching providers
Installing cert-manager
Waiting for cert-manager to be available...
Installing Provider="cluster-api" Version="v0.3.6" TargetNamespace="capi-system"
Installing Provider="bootstrap-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-bootstrap-system"
Installing Provider="control-plane-kubeadm" Version="v0.3.6" TargetNamespace="capi-kubeadm-control-plane-system"
Installing Provider="infrastructure-vsphere" Version="v0.6.6" TargetNamespace="capv-system"
Waiting for the management cluster to get ready for move...
Moving all Cluster API objects from bootstrap cluster to management cluster...
Performing move...
Discovering Cluster API objects
Moving Cluster API objects Clusters=1
Creating objects in the target cluster
Deleting objects from the source cluster
Context set for management cluster tkg-sandbox as 'tkg-sandbox-admin@tkg-sandbox'.

Management cluster created!


You can now create your first workload cluster by running the following:

  tkg create cluster [name] --kubernetes-version=[version] --plan=[plan]
```

vSphere上には次のVMが作成されています。

![image](https://user-images.githubusercontent.com/106908/93874343-2f1caf80-fd0e-11ea-879b-259985e42603.png)

`***-lb`はMaster VMに対するLBで、実体はHA Proxyです。

`tkg get management-cluster`コマンドでManagement Cluster一覧を取得できます。

```
$ tkg get management-cluster
 MANAGEMENT-CLUSTER-NAME  CONTEXT-NAME                  
 tkg-sandbox *            tkg-sandbox-admin@tkg-sandbox 
```

`tkg get cluster --include-management-cluster`コマンドでWorkload Cluster及びManagement Cluster一覧を取得できます。

```
$ tkg get cluster --include-management-cluster
   NAME         NAMESPACE   STATUS   CONTROLPLANE  WORKERS  KUBERNETES       
   tkg-sandbox  tkg-system  running  1/1           1/1      v1.18.6+vmware.1
```

Management Clusterに`kubectl`コマンドでアクセスしてみます。

```
$ kubectl config use-context tkg-sandbox-admin@tkg-sandbox
$ kubectl cluster-info
Kubernetes master is running at https://10.213.173.208:6443
KubeDNS is running at https://10.213.173.208:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
$ kubectl get node -o wide
NAME                                STATUS   ROLES    AGE   VERSION            INTERNAL-IP      EXTERNAL-IP      OS-IMAGE                 KERNEL-VERSION   CONTAINER-RUNTIME
tkg-sandbox-control-plane-p276c     Ready    master   14m   v1.18.6+vmware.1   10.213.173.209   10.213.173.209   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
tkg-sandbox-md-0-6498fc8bd4-k59gr   Ready    <none>   11m   v1.18.6+vmware.1   10.213.173.210   10.213.173.210   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4

$ kubectl get pod -A -o wide
NAMESPACE                           NAME                                                             READY   STATUS    RESTARTS   AGE     IP               NODE                                NOMINATED NODE   READINESS GATES
capi-kubeadm-bootstrap-system       capi-kubeadm-bootstrap-controller-manager-6857dfc668-6q9c2       2/2     Running   0          9m37s   100.99.203.8     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
capi-kubeadm-control-plane-system   capi-kubeadm-control-plane-controller-manager-85f4885cf5-5m5qj   2/2     Running   0          9m17s   100.114.41.134   tkg-sandbox-control-plane-p276c     <none>           <none>
capi-system                         capi-controller-manager-5df8c8fb59-9r9k2                         2/2     Running   0          9m59s   100.99.203.6     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
capi-webhook-system                 capi-controller-manager-7d8d9b87b8-ngwfc                         2/2     Running   0          10m     100.99.203.5     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
capi-webhook-system                 capi-kubeadm-bootstrap-controller-manager-dff99d987-48fh6        2/2     Running   1          9m49s   100.99.203.7     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
capi-webhook-system                 capi-kubeadm-control-plane-controller-manager-6cc995dd6c-zpj7g   2/2     Running   0          9m30s   100.99.203.9     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
capi-webhook-system                 capv-controller-manager-bfb88d6df-zkshm                          2/2     Running   1          9m5s    100.114.41.135   tkg-sandbox-control-plane-p276c     <none>           <none>
capv-system                         capv-controller-manager-7844b47584-dslbv                         2/2     Running   0          8m51s   100.114.41.136   tkg-sandbox-control-plane-p276c     <none>           <none>
cert-manager                        cert-manager-b56b4dc78-58tkh                                     1/1     Running   0          11m     100.99.203.3     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
cert-manager                        cert-manager-cainjector-6b54f84d85-cs6k7                         1/1     Running   0          11m     100.99.203.2     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
cert-manager                        cert-manager-webhook-6fbc6d7449-j2g4v                            1/1     Running   0          11m     100.99.203.4     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
kube-system                         calico-kube-controllers-54fd4b48dd-2k8d7                         1/1     Running   0          12m     100.114.41.133   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         calico-node-cx76v                                                1/1     Running   0          11m     10.213.173.210   tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
kube-system                         calico-node-kx2wm                                                1/1     Running   0          12m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         coredns-5cf78cdcc-58ssl                                          1/1     Running   0          13m     100.114.41.130   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         coredns-5cf78cdcc-z86j8                                          1/1     Running   0          13m     100.114.41.129   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         etcd-tkg-sandbox-control-plane-p276c                             1/1     Running   0          13m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         kube-apiserver-tkg-sandbox-control-plane-p276c                   1/1     Running   0          12m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         kube-controller-manager-tkg-sandbox-control-plane-p276c          1/1     Running   1          12m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         kube-proxy-h2c6p                                                 1/1     Running   0          11m     10.213.173.210   tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
kube-system                         kube-proxy-zptl6                                                 1/1     Running   0          13m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         kube-scheduler-tkg-sandbox-control-plane-p276c                   1/1     Running   0          13m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         vsphere-cloud-controller-manager-xb5gb                           1/1     Running   0          13m     10.213.173.209   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         vsphere-csi-controller-8c9b98f7f-jn4gx                           5/5     Running   0          13m     100.114.41.132   tkg-sandbox-control-plane-p276c     <none>           <none>
kube-system                         vsphere-csi-node-24mjf                                           3/3     Running   0          11m     100.99.203.1     tkg-sandbox-md-0-6498fc8bd4-k59gr   <none>           <none>
kube-system                         vsphere-csi-node-g8pcq                                           3/3     Running   0          13m     100.114.41.131   tkg-sandbox-control-plane-p276c     <none>           <none>
```

Cluster APIのリソースも見てみます。

```
$ kubectl get cluster -A
NAMESPACE    NAME          PHASE
tkg-system   tkg-sandbox   Provisioned

$ kubectl get machinedeployment -A
NAMESPACE    NAME               PHASE     REPLICAS   AVAILABLE   READY
tkg-system   tkg-sandbox-md-0   Running   1          1           1

$ kubectl get machineset -A       
NAMESPACE    NAME                          REPLICAS   AVAILABLE   READY
tkg-system   tkg-sandbox-md-0-6498fc8bd4   1          1           1

$ kubectl get machine -A   
NAMESPACE    NAME                                PROVIDERID                                       PHASE
tkg-system   tkg-sandbox-control-plane-p276c     vsphere://420680e6-f224-b13a-e260-1ee80cd2eacd   Running
tkg-system   tkg-sandbox-md-0-6498fc8bd4-k59gr   vsphere://42063041-93a6-b855-b234-97e1dda67190   Running

$ kubectl get machinehealthcheck -A
NAMESPACE    NAME          MAXUNHEALTHY   EXPECTEDMACHINES   CURRENTHEALTHY
tkg-system   tkg-sandbox   100%           1                  1

$ kubectl get kubeadmcontrolplane -A
NAMESPACE    NAME                        READY   INITIALIZED   REPLICAS   READY REPLICAS   UPDATED REPLICAS   UNAVAILABLE REPLICAS
tkg-system   tkg-sandbox-control-plane   true    true          1          1                1                  

$ kubectl get kubeadmconfigtemplate -A
NAMESPACE    NAME               AGE
tkg-system   tkg-sandbox-md-0   9m59s
```

vSphere実装のリソースも見てみます。

```
$ kubectl get vspherecluster -A 
NAMESPACE    NAME          AGE
tkg-system   tkg-sandbox   10m

$ kubectl get vspheremachine -A
NAMESPACE    NAME                              AGE
tkg-system   tkg-sandbox-control-plane-82ws2   10m
tkg-system   tkg-sandbox-worker-gg8cs          10m

$ kubectl get vspheremachinetemplate -A
NAMESPACE    NAME                        AGE
tkg-system   tkg-sandbox-control-plane   10m
tkg-system   tkg-sandbox-worker          10m
```

### Workload Clusterを作成

対象のManagement Clusterを`tkg set management-cluster`で指定します。1つしかなければ不要です。

```
$ tkg set management-cluster tkg-sandbox
The current management cluster context is switched to tkg-sandbox
```

対象となっているManagement Clusterを`tkg set management-cluster`で確認できます。`*`がついているクラスタが対象です。

```
$ tkg get management-cluster            
 MANAGEMENT-CLUSTER-NAME  CONTEXT-NAME                  
 tkg-sandbox *            tkg-sandbox-admin@tkg-sandbox 
```

`tkg create cluster`でWorkload Clusterを作成します。

```
tkg create cluster demo --plan dev
```

次のようなログが出力されます。

```
Logs of the command execution can also be found at: /var/folders/76/vg4pyy253pbgwzncmx2mb1gh0000gq/T/tkg-20200922T200429948113328.log
Validating configuration...
Creating workload cluster 'demo'...
Waiting for cluster to be initialized...
Waiting for cluster nodes to be available...

Workload cluster 'demo' created
```

vSphere上には次のVMが作成されています。

![image](https://user-images.githubusercontent.com/106908/93875271-a7d03b80-fd0f-11ea-96c5-82b01371d271.png)

`tkg get cluster`でWorkload Clusterを確認できます。

```
$ tkg get cluster
 NAME  NAMESPACE  STATUS   CONTROLPLANE  WORKERS  KUBERNETES       
 demo  default    running  1/1           1/1      v1.18.6+vmware.1 
```

Cluster APIのリソースを確認します。

```
$ kubectl get cluster -A
NAMESPACE    NAME          PHASE
default      demo          Provisioned
tkg-system   tkg-sandbox   Provisioned

$ kubectl get machinedeployment -A
NAMESPACE    NAME               PHASE     REPLICAS   AVAILABLE   READY
default      demo-md-0          Running   1          1           1
tkg-system   tkg-sandbox-md-0   Running   1          1           1

$ kubectl get machineset -A  
NAMESPACE    NAME                          REPLICAS   AVAILABLE   READY
default      demo-md-0-685646df5c          1          1           1
tkg-system   tkg-sandbox-md-0-6498fc8bd4   1          1           1

$ kubectl get machine -A   
NAMESPACE    NAME                                PROVIDERID                                       PHASE
default      demo-control-plane-wkngh            vsphere://42063032-21f6-9174-a095-f62516c4945b   Running
default      demo-md-0-685646df5c-gcb8n          vsphere://42063ac3-be34-04ce-0e9b-826065e65211   Running
tkg-system   tkg-sandbox-control-plane-p276c     vsphere://420680e6-f224-b13a-e260-1ee80cd2eacd   Running
tkg-system   tkg-sandbox-md-0-6498fc8bd4-k59gr   vsphere://42063041-93a6-b855-b234-97e1dda67190   Running

$ kubectl get machinehealthcheck -A
NAMESPACE    NAME          MAXUNHEALTHY   EXPECTEDMACHINES   CURRENTHEALTHY
default      demo          100%           1                  1
tkg-system   tkg-sandbox   100%           1                  1

$ kubectl get kubeadmcontrolplane -A
NAMESPACE    NAME                        READY   INITIALIZED   REPLICAS   READY REPLICAS   UPDATED REPLICAS   UNAVAILABLE REPLICAS
default      demo-control-plane          true    true          1          1                1                  
tkg-system   tkg-sandbox-control-plane   true    true          1          1                1  

$ kubectl get kubeadmconfigtemplate -A
NAMESPACE    NAME               AGE
default      demo-md-0          7m40s
tkg-system   tkg-sandbox-md-0   19m

$ kubectl get vspherecluster -A 
NAMESPACE    NAME          AGE
default      demo          8m5s
tkg-system   tkg-sandbox   19m

$ kubectl get vspheremachine -A
NAMESPACE    NAME                              AGE
default      demo-control-plane-xlzwf          7m21s
default      demo-worker-m7sqh                 8m13s
tkg-system   tkg-sandbox-control-plane-82ws2   19m
tkg-system   tkg-sandbox-worker-gg8cs          19m

$ kubectl get vspheremachinetemplate -A
NAMESPACE    NAME                        AGE
default      demo-control-plane          8m27s
default      demo-worker                 8m26s
tkg-system   tkg-sandbox-control-plane   20m
tkg-system   tkg-sandbox-worker          20m
```

Workload Cluster `demo`のconfigを取得して、Current Contextに設定します。

```
tkg get credentials demo
kubectl config use-context demo-admin@demo
```

Workload Cluster `demo`を確認します。

```
$ kubectl cluster-info
Kubernetes master is running at https://10.213.173.211:6443
KubeDNS is running at https://10.213.173.211:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

$ kubectl get node -o wide
NAME                         STATUS   ROLES    AGE     VERSION            INTERNAL-IP      EXTERNAL-IP      OS-IMAGE                 KERNEL-VERSION   CONTAINER-RUNTIME
demo-control-plane-wkngh     Ready    master   7m10s   v1.18.6+vmware.1   10.213.173.212   10.213.173.212   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
demo-md-0-685646df5c-gcb8n   Ready    <none>   4m58s   v1.18.6+vmware.1   10.213.173.213   10.213.173.213   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4

$ kubectl get pod -A -o wide
NAMESPACE     NAME                                               READY   STATUS    RESTARTS   AGE     IP               NODE                         NOMINATED NODE   READINESS GATES
kube-system   calico-kube-controllers-54fd4b48dd-57gdb           1/1     Running   0          5m44s   100.119.74.193   demo-control-plane-wkngh     <none>           <none>
kube-system   calico-node-7dq4r                                  1/1     Running   0          5m45s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   calico-node-tzn8b                                  1/1     Running   0          5m16s   10.213.173.213   demo-md-0-685646df5c-gcb8n   <none>           <none>
kube-system   coredns-5cf78cdcc-6vsmv                            1/1     Running   0          6m32s   100.119.74.197   demo-control-plane-wkngh     <none>           <none>
kube-system   coredns-5cf78cdcc-9w69b                            1/1     Running   0          6m32s   100.119.74.195   demo-control-plane-wkngh     <none>           <none>
kube-system   etcd-demo-control-plane-wkngh                      1/1     Running   0          6m15s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   kube-apiserver-demo-control-plane-wkngh            1/1     Running   0          5m50s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   kube-controller-manager-demo-control-plane-wkngh   1/1     Running   0          6m8s    10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   kube-proxy-dwpvl                                   1/1     Running   0          5m16s   10.213.173.213   demo-md-0-685646df5c-gcb8n   <none>           <none>
kube-system   kube-proxy-z9qqj                                   1/1     Running   0          6m32s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   kube-scheduler-demo-control-plane-wkngh            1/1     Running   0          6m11s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   vsphere-cloud-controller-manager-9m4rv             1/1     Running   0          6m32s   10.213.173.212   demo-control-plane-wkngh     <none>           <none>
kube-system   vsphere-csi-controller-8c9b98f7f-w6pb8             5/5     Running   0          6m32s   100.119.74.194   demo-control-plane-wkngh     <none>           <none>
kube-system   vsphere-csi-node-ct7sn                             3/3     Running   0          6m32s   100.119.74.196   demo-control-plane-wkngh     <none>           <none>
kube-system   vsphere-csi-node-vk5jx                             3/3     Running   0          5m16s   100.111.20.65    demo-md-0-685646df5c-gcb8n   <none>           <none>
```

### MetalLBのインストール

TKG on vSphereの場合、Workload Clusterに対するLoadBalancerが用意されていません。ここでは動作検証用に[MetalLB](https://metallb.universe.tf)を使用します。

TKGをインストールしたネットワーク内でDHCPの範囲外で空いているIPの範囲を次の`METALLB_START_IP`と`METALLB_END_IP`に設定します。Workload Cluster毎に異なる範囲を指定しないといけないことに気をつけてください。

次のコマンドを実行してMetalLBをインストールします。

```
METALLB_START_IP=10.213.173.50
METALLB_END_IP=10.213.173.70

mkdir -p metallb
wget -O metallb/namespace.yaml https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml
wget -O metallb/metallb.yaml https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml
kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)" --dry-run=client -o yaml > metallb/secret.yaml

cat > metallb/configmap.yaml << EOF
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - ${METALLB_START_IP}-${METALLB_END_IP}
EOF
kubectl apply -f metallb/namespace.yaml -f metallb/metallb.yaml -f metallb/secret.yaml -f metallb/configmap.yaml
```

Podを確認します。

```
$ kubectl get pod -n metallb-system
NAME                          READY   STATUS    RESTARTS   AGE
controller-57f648cb96-9mh99   1/1     Running   0          82s
speaker-5qxk5                 1/1     Running   0          83s
speaker-kgmw8                 1/1     Running   0          83s
```

動作確認用にサンプルアプリをデプロイします。

```
kubectl create deployment demo --image=making/hello-world --dry-run=client -o=yaml > /tmp/deployment.yaml
echo --- >> /tmp/deployment.yaml
kubectl create service loadbalancer demo --tcp=80:8080 --dry-run=client -o=yaml >> /tmp/deployment.yaml
kubectl apply -f /tmp/deployment.yaml
```

PodとServiceを確認します。

```
$ kubectl get pod,svc -l app=demo
NAME                        READY   STATUS    RESTARTS   AGE
pod/demo-6996c67686-l847v   1/1     Running   0          25s

NAME           TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)        AGE
service/demo   LoadBalancer   100.70.73.202   10.213.173.50   80:31150/TCP   25s
```

アプリにアクセスします。

```
$ curl http://10.213.173.50                
Hello World!

$ curl http://10.213.173.50/actuator/health
{"status":"UP","groups":["liveness","readiness"]}
```

デプロイしたリソースを削除します。

```
kubectl delete -f /tmp/deployment.yaml
```

### StorageClassの設定

次のようにWorkload Cluster作成直後は`StorageClass`が設定されていません。

```
$ kubectl get storageclass
No resources found in default namespace.
```

Menuから"Tags & Custom Attributes"を選択してください。

![image](https://user-images.githubusercontent.com/106908/93879364-8aeb3680-fd16-11ea-9e2f-22c6ac0038cc.png)

"CATEGORIES"を選択し、"NEW"をクリックし、

* "Category Name:"に対して`k8s-storage`を入力
* "Associatable Object Types:"に対して`Datastore`, `Datastore Cluster`を選択

してOKをクリックください。

![image](https://user-images.githubusercontent.com/106908/93883121-1adfaf00-fd1c-11ea-9438-c4d6414d6abc.png)

`k8s-storage`カテゴリが作成されます。

![image](https://user-images.githubusercontent.com/106908/93883484-92154300-fd1c-11ea-9cc1-dc2cf22624c5.png)

次に"TAGS"を選択し、"NEW"をクリックし、

* "Name:"に対して`k8s-storage`を入力
* "Category:"に対して`k8s-storage`を選択

してOKをクリックください。

![image](https://user-images.githubusercontent.com/106908/93883522-a5c0a980-fd1c-11ea-825b-91694e20fe11.png)

`k8s-storage`タグが作成されます。

![image](https://user-images.githubusercontent.com/106908/93883574-b5d88900-fd1c-11ea-8374-791b0f2fe5ba.png)

Menuから"Storage"を選択してください。

対象のDatastoreのSummary内のTagsから"Assign Tag"をクリックし、`k8s-storage`にチェックを入れ、"ASSIGN"をクリックしてください。

![image](https://user-images.githubusercontent.com/106908/93884335-baea0800-fd1d-11ea-9735-48603057e829.png)

Summary内のTagsに`k8s-storage`タグが表示されます。

![image](https://user-images.githubusercontent.com/106908/93884374-c806f700-fd1d-11ea-841e-d0b874ae1d16.png)

Menuから"Policy and Profiles"を選択してください。

![image](https://user-images.githubusercontent.com/106908/93883610-c426a500-fd1c-11ea-99be-1300130c0cf7.png)

"VM Storage Profiles"を選択し、"Create VM Storage Policy"をクリックしてください。

"Name:"に対して`k8s Storage Policy`を入力して"NEXT"をクリックしてください。

![image](https://user-images.githubusercontent.com/106908/93883736-eddfcc00-fd1c-11ea-92e2-e2da76acc144.png)

"Enable tag based placement rules"に対してチェックを入れ、"NEXT"をクリックしてください。

![image](https://user-images.githubusercontent.com/106908/93883767-f89a6100-fd1c-11ea-8c8d-dc8af0053665.png)

"Tag category"に対して`k8s-storage`を入力し、"BROWSE TAGS"をクリックして`k8s-storage`を選択し、"NEXT"をクリックしてください。

![image](https://user-images.githubusercontent.com/106908/93883811-064fe680-fd1d-11ea-8643-a6a87ad128d3.png)

Tagを設定したDatastoreがリストに表示されていることを確認し、"NEXT"をクリックしてください。

![image](https://user-images.githubusercontent.com/106908/93884474-e79e1f80-fd1d-11ea-8350-f302a4823235.png)

設定内容を確認して"FINISH"をクリックしてください。

![image](https://user-images.githubusercontent.com/106908/93884503-eff65a80-fd1d-11ea-8af2-246955fbdbfc.png)

VM Storage Policies一覧に`k8s Storage Policy`が追加されます。

![image](https://user-images.githubusercontent.com/106908/93884539-fedd0d00-fd1d-11ea-8f83-cd806265445a.png)

次に、このStorage Policyを設定した`StorageClass`オブジェクトを作成します。

次のコマンドを実行してください。

```
cat <<EOF > storageclass.yml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: standard
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: csi.vsphere.vmware.com
parameters:
  storagepolicyname: "k8s Storage Policy"
  fstype: ext3
allowVolumeExpansion: true
EOF
kubectl apply -f storageclass.yml
```

`StorageClass`一覧を確認します。

```
$ kubectl get storageclass
NAME                 PROVISIONER              RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
standard (default)   csi.vsphere.vmware.com   Delete          Immediate           true                   6s
```

動作確認のため、次の`PersistentVolumeClaim`オブジェクトを作成します。

```
cat <<EOF > /tmp/pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
EOF
kubectl apply -f /tmp/pvc.yml
```

`PersistentVolumeClaim`及び`PersistentVolume`を確認し、`STATUS`が`Bound`になっていることを確認してください。

```
$ kubectl get pv,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS   REASON   AGE
persistentvolume/pvc-77a75ab7-38cd-4a3c-a534-cbbe79d0abd1   1Gi        RWO            Delete           Bound    default/task-pv-claim   standard                5s

NAME                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/task-pv-claim   Bound    pvc-77a75ab7-38cd-4a3c-a534-cbbe79d0abd1   1Gi        RWO            standard       7s
```

vCenter上でこのVolumeを確認できます。

![image](https://user-images.githubusercontent.com/106908/93887011-18338880-fd21-11ea-857c-33314f74228e.png)

デプロイしたリソースを削除します。

```
kubectl delete -f /tmp/pvc.yml
```

### Workload Clusterのスケールアウト

`tkg scale cluster`コマンドでWorkload Clusterをスケールアウトできます。

```
tkg scale cluster demo -w 3
```

Workerが3台に増えたことが確認できます。

```
$ kubectl get node -o wide
NAME                         STATUS   ROLES    AGE    VERSION            INTERNAL-IP      EXTERNAL-IP      OS-IMAGE                 KERNEL-VERSION   CONTAINER-RUNTIME
demo-control-plane-wkngh     Ready    master   132m   v1.18.6+vmware.1   10.213.173.212   10.213.173.212   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
demo-md-0-685646df5c-gcb8n   Ready    <none>   130m   v1.18.6+vmware.1   10.213.173.213   10.213.173.213   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
demo-md-0-685646df5c-hksh9   Ready    <none>   70s    v1.18.6+vmware.1   10.213.173.215   10.213.173.215   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
demo-md-0-685646df5c-pslwf   Ready    <none>   70s    v1.18.6+vmware.1   10.213.173.214   10.213.173.214   VMware Photon OS/Linux   4.19.132-1.ph3   containerd://1.3.4
```

### Workload Clusterを削除

`demo` Workload Clusterを削除します。

```
tkg delete cluster demo -y
```

`tkg get cluster`を確認するとSTATUSが`deleting`になります。

```
$ tkg get cluster
 NAME  NAMESPACE  STATUS    CONTROLPLANE  WORKERS  KUBERNETES 
 demo  default    deleting 
```

しばらくすると`tkg get cluster`の結果から`demo`が消えます。

```
$ tkg get cluster 
 NAME  NAMESPACE  STATUS  CONTROLPLANE  WORKERS  KUBERNETES 
```

### Management Clusterの削除

次のコマンドでManagement Clusterを削除します。

```
source tkg-env.sh
tkg delete management-cluster tkg-sandbox -y
```

Management Clusterの削除はKindが使われます。

---

TKGでvSphere 6.7上にKubernetesクラスタを作成しました。

`tkg`コマンド(+ Cluster API)により一貫した手法でAWSでもvSphereでも同じようにクラスタを作成・管理できるため、
マルチクラウドでKubernetesを管理している場合は特に有用だと思います。

AzureとGCPは今後対応するようです。
